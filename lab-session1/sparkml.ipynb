{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val spark: SparkSession = SparkSession.builder().master(\"local\").appName(\"test\").getOrCreate()\n",
    "\n",
    "val people = spark.read.format(\"json\").load(\"people.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "people.select(col(\"name\"), expr(\"age + 3\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spark.implicits._\n",
    "\n",
    "people.select($\"name\", $\"age\" + 3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.where(\"age < 20\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.select(\"name\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.withColumn(\"teenager\", expr(\"age < 20\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.withColumnRenamed(\"name\", \"username\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.select(count(\"age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.select(first(\"name\"), last(\"age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.select(avg(\"age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.groupBy(\"name\").agg(count(\"age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val t1 = spark.createDataFrame(Seq((0, \"a\", 0), (1, \"b\", 1), (2, \"c\", 1))).toDF(\"num\", \"name\", \"id\")\n",
    "val t2 = spark.createDataFrame(Seq((0, \"x\"), (1, \"y\"), (2, \"z\"))).toDF(\"id\", \"group\")\n",
    "\n",
    "val joinExpression = t1.col(\"id\") === t2.col(\"id\")\n",
    "var joinType = \"inner\"\n",
    "\n",
    "t1.join(t2, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.udf\n",
    "val df = spark.createDataFrame(Seq((0, \"hello\"), (1, \"world\"))).toDF(\"id\", \"text\")\n",
    "val upper: String => String = _.toUpperCase\n",
    "val upperUDF = spark.udf.register(\"upper\", upper)\n",
    "df.withColumn(\"upper\", upperUDF(col(\"text\"))).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing Example - Individual Stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "case class Article(id: Long, topic: String, text: String)\n",
    "\n",
    "val articles = spark.createDataFrame(Seq(\n",
    "    Article(0, \"sci.math\", \"Hello, Math!\"),\n",
    "    Article(1, \"alt.religion\", \"Hello, Religion!\"),\n",
    "    Article(2, \"sci.physics\", \"Hello, Physics!\"),\n",
    "    Article(3, \"sci.math\", \"Hello, Math Revised!\"),\n",
    "    Article(4, \"sci.math\", \"Better Math\"),\n",
    "    Article(5, \"alt.religion\", \"TGIF\"))).toDF\n",
    "\n",
    "articles.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val topic2Label: Boolean => Double = x => if (x) 1 else 0\n",
    "\n",
    "val toLabel = spark.udf.register(\"topic2Label\", topic2Label)\n",
    "\n",
    "val labelled = articles.withColumn(\"label\", toLabel($\"topic\".like(\"sci%\"))).cache\n",
    "\n",
    "labelled.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.Tokenizer\n",
    "import org.apache.spark.ml.feature.RegexTokenizer\n",
    "\n",
    "val tokenizer = new RegexTokenizer().setInputCol(\"text\").setOutputCol(\"words\")\n",
    "\n",
    "val tokenized = tokenizer.transform(labelled)\n",
    "\n",
    "tokenized.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.HashingTF\n",
    "\n",
    "val hashingTF = new HashingTF().setInputCol(tokenizer.getOutputCol)\n",
    "    .setOutputCol(\"features\")\n",
    "    .setNumFeatures(5000)\n",
    "\n",
    "val hashed = hashingTF.transform(tokenized)\n",
    "\n",
    "hashed.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val Array(trainDF, testDF) = hashed.randomSplit(Array(0.8, 0.2))\n",
    "\n",
    "trainDF.show\n",
    "\n",
    "testDF.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "\n",
    "val lr = new LogisticRegression().setMaxIter(20).setRegParam(0.01)\n",
    "\n",
    "val model = lr.fit(trainDF)\n",
    "\n",
    "val pred = model.transform(testDF).select(\"topic\", \"label\", \"prediction\")\n",
    "\n",
    "pred.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing Example - Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val Array(trainDF2, testDF2) = labelled.randomSplit(Array(0.8, 0.2))\n",
    "\n",
    "trainDF2.show\n",
    "\n",
    "testDF2.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(tokenizer, hashingTF, lr))\n",
    "\n",
    "val model2 = pipeline.fit(trainDF2)\n",
    "\n",
    "val pred = model2.transform(testDF2).select(\"topic\", \"label\", \"prediction\")\n",
    "\n",
    "pred.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering - Vector Assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "case class Nums(val1: Long, val2: Long, val3: Long)\n",
    "\n",
    "val numsDF = spark.createDataFrame(Seq(Nums(1, 2, 3), Nums(4, 5, 6), Nums(7, 8, 9))).toDF\n",
    "\n",
    "val va = new VectorAssembler().setInputCols(Array(\"val1\", \"val2\", \"val3\")).setOutputCol(\"features\")\n",
    "\n",
    "va.transform(numsDF).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering - Continues Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.Bucketizer\n",
    "\n",
    "val contDF = spark.range(20).selectExpr(\"cast(id as double)\")\n",
    "\n",
    "val bucketBorders = Array(-1.0, 5.0, 10.0, 15.0, 20.0)\n",
    "val bucketer = new Bucketizer().setSplits(bucketBorders).setInputCol(\"id\")\n",
    "\n",
    "bucketer.transform(contDF).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "case class Nums(val1: Long, val2: Long, val3: Long)\n",
    "\n",
    "val numsDF = spark.createDataFrame(Seq(Nums(1, 2, 3), Nums(4, 5, 6), Nums(7, 8, 9))).toDF\n",
    "\n",
    "val va = new VectorAssembler().setInputCols(Array(\"val1\", \"val2\", \"val3\")).setOutputCol(\"features\")\n",
    "\n",
    "val nums = va.transform(numsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.StandardScaler\n",
    "\n",
    "val scaler = new StandardScaler().setInputCol(\"features\").setOutputCol(\"scaled\")\n",
    "\n",
    "scaler.fit(nums).transform(nums).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "case class Nums(val1: Long, val2: Long, val3: Long)\n",
    "\n",
    "val numsDF = spark.createDataFrame(Seq(Nums(1, 2, 3), Nums(4, 5, 6), Nums(7, 8, 9))).toDF\n",
    "\n",
    "val va = new VectorAssembler().setInputCols(Array(\"val1\", \"val2\", \"val3\")).setOutputCol(\"features\")\n",
    "\n",
    "val nums = va.transform(numsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering - Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val simpleDF = spark.read.json(\"simple-ml.json\")\n",
    "\n",
    "simpleDF.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "\n",
    "val lblIndxr = new StringIndexer().setInputCol(\"lab\").setOutputCol(\"labelInd\")\n",
    "\n",
    "val idxRes = lblIndxr.fit(simpleDF).transform(simpleDF)\n",
    "\n",
    "idxRes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.IndexToString\n",
    "\n",
    "val labelReverse = new IndexToString().setInputCol(\"labelInd\").setOutputCol(\"original\")\n",
    "\n",
    "labelReverse.transform(idxRes).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.OneHotEncoder\n",
    "\n",
    "val lblIndxr = new StringIndexer().setInputCol(\"color\").setOutputCol(\"colorInd\")\n",
    "\n",
    "val colorLab = lblIndxr.fit(simpleDF).transform(simpleDF.select(\"color\"))\n",
    "\n",
    "val ohe = new OneHotEncoder().setInputCol(\"colorInd\").setOutputCol(\"one-hot\")\n",
    "\n",
    "ohe.transform(colorLab).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering - Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val sales = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"sales.csv\").where(\"Description IS NOT NULL\")\n",
    "\n",
    "sales.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.Tokenizer\n",
    "\n",
    "val tkn = new Tokenizer().setInputCol(\"Description\").setOutputCol(\"DescOut\")\n",
    "\n",
    "val tokenized = tkn.transform(sales.select(\"Description\"))\n",
    "\n",
    "tokenized.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.StopWordsRemover\n",
    "\n",
    "val df = spark.createDataFrame(Seq((0, Seq(\"I\", \"saw\", \"the\", \"red\", \"balloon\")),\n",
    "                                   (1, Seq(\"Mary\", \"had\", \"a\", \"little\", \"lamb\")))).toDF(\"id\", \"raw\")\n",
    "\n",
    "val englishStopWords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "\n",
    "val stops = new StopWordsRemover().setStopWords(englishStopWords).setInputCol(\"raw\").setOutputCol(\"WithoutStops\")\n",
    "\n",
    "stops.transform(df).show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.CountVectorizer\n",
    "\n",
    "val df = spark.createDataFrame(Seq((0, Array(\"a\", \"b\", \"c\")),\n",
    "                                   (1, Array(\"a\", \"b\", \"b\", \"c\", \"a\")))).toDF(\"id\", \"words\")\n",
    "\n",
    "val cvModel = new CountVectorizer().setInputCol(\"words\").setOutputCol(\"features\").setVocabSize(3).setMinDF(2)\n",
    "\n",
    "val fittedCV = cvModel.fit(df)\n",
    "\n",
    "fittedCV.transform(df).show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val data = spark.read.format(\"libsvm\").load(\"data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "\n",
    "val lr = new LinearRegression().setMaxIter(10)\n",
    "val lrModel = lr.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(s\"Coefficients: ${lrModel.coefficients} Intercept: ${lrModel.intercept}\")\n",
    "val trainingSummary = lrModel.summary\n",
    "println(s\"RMSE: ${trainingSummary.rootMeanSquaredError}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case class cancer(x1: Long, y: Long)\n",
    "val trainData = spark.createDataFrame(Seq(cancer(330, 1), cancer(120, 0), cancer(400, 1))).toDF\n",
    "val testData = spark.createDataFrame(Seq(cancer(500, 0))).toDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "val va = new VectorAssembler().setInputCols(Array(\"x1\")).setOutputCol(\"features\")\n",
    "val train = va.transform(trainData)\n",
    "val test = va.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "\n",
    "val lr = new LogisticRegression().setFeaturesCol(\"features\").setLabelCol(\"y\")\n",
    ".setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "val lrModel = lr.fit(train)\n",
    "lrModel.transform(test).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val training = spark.read.format(\"libsvm\").load(\"multiclass_data.txt\")\n",
    "\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "val lr = new LogisticRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "val lrModel = lr.fit(training)\n",
    "\n",
    "println(s\"Coefficients: \\n${lrModel.coefficientMatrix}\")\n",
    "println(s\"Intercepts: \\n${lrModel.interceptVector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
